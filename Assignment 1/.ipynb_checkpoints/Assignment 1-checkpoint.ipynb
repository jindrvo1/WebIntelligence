{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b833a6d88f74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Import top-level functionality into top-level namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatstruct\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/collocations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContingencyMeasures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrigramAssocMeasures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspearman\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mranks_from_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspearman_correlation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m from nltk.metrics.scores import          (accuracy, precision, recall, f_measure,\n\u001b[0m\u001b[1;32m     17\u001b[0m                                           log_likelihood, approxrand)\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusionmatrix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/metrics/scores.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbetai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mbetai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_distn_infrastructure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,\n\u001b[0m\u001b[1;32m     11\u001b[0m                                     rv_frozen)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# for root finding for discrete distribution ppf, and max likelihood estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/optimize/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_minimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mminpack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# unconstrained minimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0misolve\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdsolve\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/isolve/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from info import __doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0miterative\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mminres\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlgmres\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlgmres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/isolve/iterative.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_iterative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlsplit\n",
    "from queue import Queue\n",
    "import re\n",
    "from url_normalize import url_normalize\n",
    "import urllib3\n",
    "from glob import glob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.core.display import display, HTML\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable InsecureRequestWarning\n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple URL normalization\n",
    "# Converts relative URLs to absolute and converts to lowercase, then applies url-normalize\n",
    "# See (https://pypi.org/project/url-normalize/)\n",
    "\n",
    "def norm_url(url, base):\n",
    "    if url[0] == '/':\n",
    "        return url_normalize(urljoin(base, url).lower())\n",
    "    return url_normalize(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the base of a URL\n",
    "\n",
    "def get_base_url(url):\n",
    "    split = urlsplit(url)\n",
    "    base = split.scheme + '://' + split.netloc + '/'\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for outlinks in a HTML page\n",
    "\n",
    "def find_outlinks(soup):\n",
    "    # URL of the file is saved as the first comment of the HTML file.\n",
    "    base = get_base_url(soup.findAll(text = lambda text: isinstance(text, Comment))[0])\n",
    "    \n",
    "    all_urls = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "    \n",
    "    # Selects only URLs that are either absolute or relative to the domain.\n",
    "    selected_urls = [u for u in all_urls if len(u) > 1 and (u[0] == '/' and u[1] != '/' or u[0] == 'h')]\n",
    "    \n",
    "    # Normalizes selected URLs.\n",
    "    all_urls_normalized = [norm_url(u, base) for u in selected_urls]\n",
    "    \n",
    "    return all_urls_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds robots.txt and processes its rules\n",
    "\n",
    "def process_robots(url):\n",
    "    base = get_base_url(url)\n",
    "    robots_file = base + 'robots.txt'\n",
    "    req = requests.get(robots_file)\n",
    "    \n",
    "    # If robots.txt cannot be processes, no rules are applied\n",
    "    if req.status_code != 200:\n",
    "        return []\n",
    "    \n",
    "    robots = req.text.split('\\n')\n",
    "    \n",
    "    # Only check for 'User-agent: *'\n",
    "    if 'User-agent: *' not in robots:\n",
    "        return []\n",
    "    \n",
    "    robots = robots[robots.index('User-agent: *')+1:]\n",
    "\n",
    "    # Retrieve all Disallow rules\n",
    "    rules = []\n",
    "    for rule in robots:\n",
    "        if rule.startswith('Disallow: '):\n",
    "            rules.append(rule.split(' ')[1])\n",
    "        if rule.startswith('User-agent: '):\n",
    "            break\n",
    "\n",
    "    # Normalize all URLs in the rules\n",
    "    rules_normalized = [norm_url(rule, base) for rule in rules]\n",
    "    \n",
    "    return rules_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for visible text only\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    elif element == '\\n' or element == ' ':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_shingles(soup, n=4):\n",
    "    # Extract all textual data\n",
    "    data = soup.findAll(text=True)\n",
    "    \n",
    "    # Filter out comments, scripts, etc.\n",
    "    texts = filter(visible, data)\n",
    "    # Remove whitespaces\n",
    "    texts = [str(e).strip() for e in texts]\n",
    "    \n",
    "    # Convert to list of words\n",
    "    words = [text.split() for text in texts]\n",
    "    words = [w for sub in words for w in sub]\n",
    "    \n",
    "    shingles = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        shingles.append(tuple(words[i:i+n]))\n",
    "        \n",
    "    return set(shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_duplicate(shingles1, shingles2, threshold=0.8):\n",
    "    union = len(set(list(shingles1) + list(shingles2)))\n",
    "    overlap = len(list(shingles1) + list(shingles2)) - union\n",
    "\n",
    "    return overlap/union > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "n_pages = 100\n",
    "\n",
    "q = Queue()\n",
    "seed_url = \"https://en.wikipedia.org/wiki/Alexandria_Ocasio-Cortez\"\n",
    "\n",
    "q.put(seed_url)\n",
    "\n",
    "# Save texts of processed pages for near-duplication detection\n",
    "processed = []\n",
    "\n",
    "while i <= n_pages:\n",
    "    # Get URL from queue\n",
    "    url = q.get()\n",
    "\n",
    "    # Make a request to URL\n",
    "    try:\n",
    "        req = requests.get(url, verify=False, timeout=5)\n",
    "    except Exception:\n",
    "        continue\n",
    "    \n",
    "    # Check if page exists and is accessible\n",
    "    if req.status_code != 200:\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(req.text)\n",
    "    \n",
    "    # Extract 4-shingles out of textual data\n",
    "    shingles = extract_shingles(soup, n=4)\n",
    "    \n",
    "    # Check for near duplicates among processed files\n",
    "    if any([near_duplicate(shingles, processed_file, threshold=0.8) for processed_file in processed]):\n",
    "        continue\n",
    "    \n",
    "    # Save shingles for near-duplicate detection\n",
    "    processed.append(shingles)\n",
    "    \n",
    "    # Insert the URL as a comment on the first line of the created file\n",
    "    soup.insert(0, '\\n')\n",
    "    soup.insert(0, Comment(url))\n",
    "    \n",
    "    # Save the HTML of retrieved file\n",
    "    with open('Pages/{}'.format('{}.html'.format(i)), 'wb+') as file:\n",
    "        file.write(soup.prettify('utf-8'))\n",
    "                \n",
    "    # Find all outlinks in the file\n",
    "    urls = find_outlinks(soup)\n",
    "    # Retrieve rules from robots.txt\n",
    "    rules = process_robots(url)\n",
    "    \n",
    "    # Use only outlinks that aren't contradictory to rules from robots.txt\n",
    "    urls_to_add = []\n",
    "    for u in urls:\n",
    "        for rule in rules:\n",
    "            if rule.startswith(u):\n",
    "                continue\n",
    "        urls_to_add.append(u)\n",
    "    \n",
    "    # Put filtered outlinks to queue\n",
    "    for u in urls_to_add:\n",
    "        q.put(u)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corners cut:\n",
    "- Some outlinks are discarded for easier processing\n",
    "- Crawler cares only about 'Disallow' rules in robots.txt for any user-agent\n",
    "- Near-duplicate detection is done using Jaccard similarity without using super-shingles\n",
    "- URLs are added into queue without reprioritizing, Mercator's scheme isn't implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for visible text only\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    elif element == '\\n' or element == ' ':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def tokenize_text(soup):\n",
    "    # Extract all textual data\n",
    "    data = soup.findAll(text=True)\n",
    "    \n",
    "    # Filter out comments, scripts, etc.\n",
    "    texts = filter(visible, data)\n",
    "    # Remove whitespaces\n",
    "    texts = [word_tokenize(str(e).strip()) for e in texts]\n",
    "    \n",
    "    # Convert to list of words\n",
    "    words = [w.lower() for sub in texts for w in sub]\n",
    "    \n",
    "    # Remove words of length one\n",
    "    words = [w for w in words if len(w) > 1]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files crawled in previous step\n",
    "files = glob('Pages/*')\n",
    "\n",
    "docterm = {}\n",
    "words = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        # Scan file content\n",
    "        soup = BeautifulSoup(f)\n",
    "\n",
    "        # Tokenize text\n",
    "        tokenized = tokenize_text(soup)\n",
    "\n",
    "        # Prepare english stoplist\n",
    "        stopwords_list = stopwords.words('english')\n",
    "\n",
    "        # Filter all words according to stoplist\n",
    "        filtered = [word for word in tokenized if word not in stopwords_list]\n",
    "\n",
    "        # Stem the rest of the words\n",
    "        stemmed = [ps.stem(word) for word in filtered]\n",
    "\n",
    "        # Save the list of stemmed words of the file\n",
    "        docterm[file.split('/')[-1].split('.')[0]] = stemmed\n",
    "        \n",
    "        # Keep all the seen words\n",
    "        words = words + stemmed\n",
    "\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final term-document matrix\n",
    "\n",
    "termdoc = {}\n",
    "\n",
    "for word in words:\n",
    "    termdoc[word] = []\n",
    "    \n",
    "    for id, dictionary in docterm.items():\n",
    "        if word in dictionary:\n",
    "            termdoc[word].append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the term-document matrix to a file\n",
    "\n",
    "output_file = 'BinaryInvertedIndex'\n",
    "\n",
    "with open(output_file, 'w+') as f:\n",
    "    for term, documents in termdoc.items():\n",
    "        f.write('{}: {}\\n'.format(term, documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corners cut:\n",
    "- Tokenizing doesn't check for language, encoding and assumes HTML format\n",
    "- No normalization is performed\n",
    "- Synonyms and homonyms aren't explicitly taken care of\n",
    "- Inverted index is implemented without positional indexes, i.e. doesn't explicitly handle bi-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ranking (content-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Creating multidimensional vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for visible text only\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    elif element == '\\n' or element == ' ':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def tokenize_text(soup):\n",
    "    # Extract all textual data\n",
    "    data = soup.findAll(text=True)\n",
    "    \n",
    "    # Filter out comments, scripts, etc.\n",
    "    texts = filter(visible, data)\n",
    "    # Remove whitespaces\n",
    "    texts = [word_tokenize(str(e).strip()) for e in texts]\n",
    "    \n",
    "    # Convert to list of words\n",
    "    words = [w.lower() for sub in texts for w in sub]\n",
    "    \n",
    "    # Remove words of length one\n",
    "    words = [w for w in words if len(w) > 1]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files crawled in previous step\n",
    "files = glob('Pages/*')\n",
    "\n",
    "docterm = {}\n",
    "words = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        # Scan file content\n",
    "        soup = BeautifulSoup(f)\n",
    "\n",
    "        # Tokenize text\n",
    "        tokenized = tokenize_text(soup)\n",
    "\n",
    "        # Prepare english stoplist\n",
    "        stopwords_list = stopwords.words('english')\n",
    "\n",
    "        # Filter all words according to stoplist\n",
    "        filtered = [word for word in tokenized if word not in stopwords_list]\n",
    "\n",
    "        # Stem the rest of the words\n",
    "        stemmed = [ps.stem(word) for word in filtered]\n",
    "\n",
    "        # Save the list of stemmed words of the file\n",
    "        docterm[file.split('/')[-1].split('.')[0]] = stemmed\n",
    "        \n",
    "        # Keep all the seen words\n",
    "        words = words + stemmed\n",
    "\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final term-document matrix\n",
    "\n",
    "termdoc = {}\n",
    "\n",
    "for word in words:\n",
    "    termdoc[word] = {}\n",
    "    \n",
    "    for id, dictionary in docterm.items():\n",
    "        if word in dictionary:\n",
    "            # Assign ID of the document with the count of the word\n",
    "            termdoc[word][id] = dictionary.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "N = len(files)\n",
    "# Document frequency of terms\n",
    "df_t = {k: len(v) for k, v in termdoc.items()}\n",
    "\n",
    "# Inverted document frequency of terms\n",
    "idf_t = {k: np.log10(N/v) for k, v in df_t.items()}\n",
    "\n",
    "# Recalculate term frequency\n",
    "tf_td = {}\n",
    "\n",
    "for term in termdoc.keys():\n",
    "    tf_td[term] = {k: (1+np.log10(v)) for k, v in termdoc[term].items()}\n",
    "\n",
    "# Calculate tf-idf\n",
    "tfidf = {}\n",
    "\n",
    "for term in tf_td.keys():\n",
    "    tfidf[term] = {k: (v*idf_t[term]) for k, v in tf_td[term].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Representing queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(q):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(q)\n",
    "    \n",
    "    # Apply stoplist\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    tokens_stopped = [t for t in tokens if t not in stopwords_list]\n",
    "    \n",
    "    # Stem\n",
    "    ps = PorterStemmer()\n",
    "    stems = [ps.stem(t) for t in tokens_stopped]\n",
    "    \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Alexandria Ocasio-Cortez'\n",
    "\n",
    "# Tokenize, stem and remove stopwords from query\n",
    "processed = process_query(query)\n",
    "\n",
    "# Term count\n",
    "term_count = {k: (1+np.log10(processed.count(k))) for k in processed}\n",
    "# Term weight basedd on idf_t\n",
    "term_weight = {k: ((v*idf_t[k]) if k in idf_t else 0) for k, v in term_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors for computing cosine similarity\n",
    "\n",
    "N = len(files)\n",
    "docterm_tfidf = {}\n",
    "\n",
    "for i in range(1, N+1):\n",
    "    docterm_tfidf[str(i)] = {k: (tfidf[k][str(i)] if str(i) in tfidf[k] else 0) for k in words}\n",
    "\n",
    "query_tfidf = {k: (term_weight[k] if k in term_weight else 0) for k in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Finding top-$k$ results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute cosine similarity between all documents and searched query\n",
    "\n",
    "similarities = cosine_similarity(\n",
    "    [list(v.values()) for v in docterm_tfidf.values()], \n",
    "    np.array(list(query_tfidf.values())).reshape(1, -1)\n",
    ")\n",
    "\n",
    "# Index similarities with documents IDs\n",
    "similarities_indexed = {str(i): similarities[i-1][0] for i in range(1, N+1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of results to return\n",
    "k = 10\n",
    "\n",
    "# All results sorted\n",
    "results = OrderedDict(sorted(similarities_indexed.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Top-k results sorted\n",
    "top_k_results = list(results.items())[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "\n",
    "display(HTML('<b>Best result:</b>'))\n",
    "print('Pages/{}.html'.format(top_k_results[0][0]))\n",
    "display(HTML('<b>Other good results:</b>'))\n",
    "for page in top_k_results[1:]:\n",
    "    print('Pages/{}.html'.format(page[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(by considering only docs containing atleast one query term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(files)\n",
    "docterm_tfidf_p = {}\n",
    "\n",
    "# Skips documents that don't contain any query term\n",
    "for word in processed:\n",
    "    for i in range(1, N+1):\n",
    "        if str(i) in tfidf[word]:\n",
    "            docterm_tfidf_p[str(i)] = {k: (tfidf[k][str(i)] if str(i) in tfidf[k] else 0) for k in words}\n",
    "\n",
    "query_tfidf = {k: (term_weight[k] if k in term_weight else 0) for k in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_p = cosine_similarity(\n",
    "    [list(v.values()) for v in docterm_tfidf_p.values()], \n",
    "    np.array(list(query_tfidf.values())).reshape(1, -1)\n",
    ")\n",
    "\n",
    "# Index similarities with documents IDs\n",
    "similarities_indexed_p = {list(docterm_tfidf_p.keys())[i]: similarities_p[i][0] for i in range(0, len(similarities_p))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of results to return\n",
    "k = 10\n",
    "\n",
    "# All results sorted\n",
    "results = OrderedDict(sorted(similarities_indexed.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Top-k results sorted\n",
    "top_k_results = list(results.items())[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Results are the same as without pruning\n",
    "\n",
    "display(HTML('<b>Best result:</b>'))\n",
    "print('Pages/{}.html'.format(top_k_results[0][0]))\n",
    "display(HTML('<b>Other good results:</b>'))\n",
    "for page in top_k_results[1:]:\n",
    "    print('Pages/{}.html'.format(page[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ranking (link-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Calculating PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('Pages/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_anchors_and_protocol(link):\n",
    "    if '#' in link:\n",
    "        link = link[:link.index('#')]\n",
    "    if '://' in link:\n",
    "        link = link[link.find('://') + len('://'):]\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save URLs of files to build a graph\n",
    "\n",
    "urls = {}\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        soup = BeautifulSoup(f)\n",
    "        urls[remove_anchors_and_protocol(soup.findAll(text = lambda text: isinstance(text, Comment))[0])] = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directed graph\n",
    "\n",
    "g = nx.DiGraph()\n",
    "g.add_nodes_from(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill graph with edges\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        soup = BeautifulSoup(f)\n",
    "        outlinks = find_outlinks(soup)\n",
    "        no_anchors = [remove_anchors_and_protocol(link) for link in outlinks]\n",
    "\n",
    "        for link in no_anchors:\n",
    "            if link in urls:\n",
    "                g.add_edge(file, urls[link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PageRank on the graph\n",
    "pr = nx.pagerank(g, alpha=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort by PageRank and show top-k\n",
    "\n",
    "k = 10\n",
    "\n",
    "pr_sorted = OrderedDict(sorted(pr.items(), key=lambda x: x[1], reverse=True))\n",
    "top_k_pr = list(pr_sorted.items())[:k]\n",
    "\n",
    "display(HTML('<b>PageRank:</b>'))\n",
    "for page, pr_val in top_k_pr:\n",
    "    print('{}: {}'.format(page, pr_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Finding top-$k$ results considering PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Alexandria Ocasio-Cortez'\n",
    "\n",
    "# Tokenize, stem and remove stopwords from query\n",
    "processed = process_query(query)\n",
    "\n",
    "# Term count\n",
    "term_count = {k: (1+np.log10(processed.count(k))) for k in processed}\n",
    "# Term weight basedd on idf_t\n",
    "term_weight = {k: ((v*idf_t[k]) if k in idf_t else 0) for k, v in term_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(files)\n",
    "docterm_tfidf_p = {}\n",
    "\n",
    "# Skips documents that don't contain any query term\n",
    "for word in processed:\n",
    "    for i in range(1, N+1):\n",
    "        if str(i) in tfidf_norm[word]:\n",
    "            docterm_tfidf_p[str(i)] = {k: (tfidf[k][str(i)] if str(i) in tfidf[k] else 0) for k in words}\n",
    "\n",
    "query_tfidf = {k: (term_weight[k] if k in term_weight else 0) for k in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_p = cosine_similarity(\n",
    "    [list(v.values()) for v in docterm_tfidf_p.values()], \n",
    "    np.array(list(query_tfidf.values())).reshape(1, -1)\n",
    ")\n",
    "\n",
    "# Index similarities with documents IDs\n",
    "similarities_indexed_p = {list(docterm_tfidf_p.keys())[i]: similarities_p[i][0] for i in range(0, len(similarities_p))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize similarities to have same weight as PageRank\n",
    "\n",
    "sim_max = max(list(similarities_indexed_p.values()))\n",
    "sim_min = min(list(similarities_indexed_p.values()))\n",
    "\n",
    "sim_norm = {'Pages/{}.html'.format(k): (v-sim_min)/(sim_max-sim_min) for k, v in similarities_indexed_p.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune and normalize PageRanks\n",
    "\n",
    "pr_pruned = {k: v for k, v in pr.items() if k in sim_norm.keys()}\n",
    "\n",
    "pr_max = max(list(pr_pruned.values()))\n",
    "pr_min = min(list(pr_pruned.values()))\n",
    "\n",
    "pr_norm = {k: (v-pr_min)/(pr_max-pr_min) for k, v in pr_pruned.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PageRank + Similarity\n",
    "sim_pr = {k: (v+pr_norm[k])/2 for k, v in sim_norm.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top-k results\n",
    "\n",
    "k = 10\n",
    "\n",
    "sim_pr_sorted = OrderedDict(sorted(sim_pr.items(), key=lambda x: x[1], reverse=True))\n",
    "top_k_sim_pr = list(sim_pr_sorted.items())[:k]\n",
    "\n",
    "display(HTML('<b>Best result:</b>'))\n",
    "print(top_k_sim_pr[0][0])\n",
    "display(HTML('<b>Other good results:</b>'))\n",
    "for page in top_k_sim_pr[1:]:\n",
    "    print(page[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Alexandria Ocasio-Cortez'\n",
    "\n",
    "# Tokenize, stem and remove stopwords from query\n",
    "processed = process_query(query)\n",
    "\n",
    "# Term count\n",
    "term_count = {k: (1+np.log10(processed.count(k))) for k in processed}\n",
    "# Term weight basedd on idf_t\n",
    "term_weight = {k: ((v*idf_t[k]) if k in idf_t else 0) for k, v in term_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(files)\n",
    "docterm_tfidf_p = {}\n",
    "\n",
    "# Skips documents that don't contain any query term\n",
    "for word in processed:\n",
    "    for i in range(1, N+1):\n",
    "        if str(i) in tfidf_norm[word]:\n",
    "            docterm_tfidf_p[str(i)] = {k: (tfidf[k][str(i)] if str(i) in tfidf[k] else 0) for k in words}\n",
    "\n",
    "query_tfidf = {k: (term_weight[k] if k in term_weight else 0) for k in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_p = cosine_similarity(\n",
    "    [list(v.values()) for v in docterm_tfidf_p.values()], \n",
    "    np.array(list(query_tfidf.values())).reshape(1, -1)\n",
    ")\n",
    "\n",
    "# Index similarities with documents IDs\n",
    "similarities_indexed_p = {'Pages/{}.html'.format(list(docterm_tfidf_p.keys())[i]): similarities_p[i][0] for i in range(0, len(similarities_p))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of results to return\n",
    "t = 15\n",
    "\n",
    "# All results sorted\n",
    "results = OrderedDict(sorted(similarities_indexed_p.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Root sset\n",
    "top_t_results = list(results.items())[:t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only URLs that are in the root set\n",
    "urls_p = {k: v for k, v in urls.items() if v in [res[0] for res in top_t_results]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.DiGraph()\n",
    "g.add_nodes_from([res[0] for res in top_t_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add neighbours to create base set\n",
    "\n",
    "for file in [res[0] for res in top_t_results]:\n",
    "    with open(file, 'r') as f:\n",
    "        soup = BeautifulSoup(f)\n",
    "        outlinks = find_outlinks(soup)\n",
    "        no_anchors = [remove_anchors_and_protocol(link) for link in outlinks]\n",
    "\n",
    "        for link in no_anchors:\n",
    "            if link in urls:\n",
    "                urls_p[link] = urls[link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in urls.items():\n",
    "    with open(v, 'r') as f:\n",
    "        soup = BeautifulSoup(f)\n",
    "        outlinks = find_outlinks(soup)\n",
    "        no_anchors = [remove_anchors_and_protocol(link) for link in outlinks]\n",
    "\n",
    "        for link in no_anchors:\n",
    "            if link in urls_p:\n",
    "                g.add_edge(v, urls_p[link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HITS on the graph\n",
    "h, a = nx.hits(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and print top-k authorities\n",
    "\n",
    "k = 10\n",
    "\n",
    "a_sorted = OrderedDict(sorted(a.items(), key=lambda x: x[1], reverse=True))\n",
    "top_k_a = list(a_sorted.items())[:k]\n",
    "    \n",
    "display(HTML('<b>Authorities:</b>'))\n",
    "for page, a_val in top_k_a:\n",
    "    print('{}: {}'.format(page, a_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sort and print top-k hubs\n",
    "\n",
    "k = 10\n",
    "\n",
    "h_sorted = OrderedDict(sorted(h.items(), key=lambda x: x[1], reverse=True))\n",
    "top_k_h = list(h_sorted.items())[:k]\n",
    "\n",
    "display(HTML('<b>Hubs:</b>'))\n",
    "for page, h_val in top_k_h:\n",
    "    print('{}: {}'.format(page, h_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
