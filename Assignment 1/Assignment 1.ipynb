{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlsplit\n",
    "from queue import Queue\n",
    "import re\n",
    "from url_normalize import url_normalize\n",
    "import urllib3\n",
    "from glob import glob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.core.display import display, HTML\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable InsecureRequestWarning\n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple URL normalization\n",
    "# Converts relative URLs to absolute and converts to lowercase, then applies url-normalize\n",
    "# See (https://pypi.org/project/url-normalize/)\n",
    "\n",
    "def norm_url(url, base):\n",
    "    if url[0] == '/':\n",
    "        return url_normalize(urljoin(base, url).lower())\n",
    "    return url_normalize(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the base of a URL\n",
    "\n",
    "def get_base_url(url):\n",
    "    split = urlsplit(url)\n",
    "    base = split.scheme + '://' + split.netloc + '/'\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for outlinks in a HTML page\n",
    "\n",
    "def find_outlinks(soup):\n",
    "    # URL of the file is saved as the first comment of the HTML file.\n",
    "    base = get_base_url(soup.findAll(text = lambda text: isinstance(text, Comment))[0])\n",
    "    \n",
    "    all_urls = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "    \n",
    "    # Selects only URLs that are either absolute or relative to the domain.\n",
    "    selected_urls = [u for u in all_urls if len(u) > 1 and (u[0] == '/' and u[1] != '/' or u[0] == 'h')]\n",
    "    \n",
    "    # Normalizes selected URLs.\n",
    "    all_urls_normalized = [norm_url(u, base) for u in selected_urls]\n",
    "    \n",
    "    return all_urls_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds robots.txt and processes its rules\n",
    "\n",
    "def process_robots(url):\n",
    "    base = get_base_url(url)\n",
    "    robots_file = base + 'robots.txt'\n",
    "    req = requests.get(robots_file)\n",
    "    \n",
    "    # If robots.txt cannot be processes, no rules are applied\n",
    "    if req.status_code != 200:\n",
    "        return []\n",
    "    \n",
    "    robots = req.text.split('\\n')\n",
    "    \n",
    "    # Only check for 'User-agent: *'\n",
    "    if 'User-agent: *' not in robots:\n",
    "        return []\n",
    "    \n",
    "    robots = robots[robots.index('User-agent: *')+1:]\n",
    "\n",
    "    # Retrieve all Disallow rules\n",
    "    rules = []\n",
    "    for rule in robots:\n",
    "        if rule.startswith('Disallow: '):\n",
    "            rules.append(rule.split(' ')[1])\n",
    "        if rule.startswith('User-agent: '):\n",
    "            break\n",
    "\n",
    "    # Normalize all URLs in the rules\n",
    "    rules_normalized = [norm_url(rule, base) for rule in rules]\n",
    "    \n",
    "    return rules_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for visible text only\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    elif element == '\\n' or element == ' ':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_shingles(soup, n=4):\n",
    "    # Extract all textual data\n",
    "    data = soup.findAll(text=True)\n",
    "    \n",
    "    # Filter out comments, scripts, etc.\n",
    "    texts = filter(visible, data)\n",
    "    # Remove whitespaces\n",
    "    texts = [str(e).strip() for e in texts]\n",
    "    \n",
    "    # Convert to list of words\n",
    "    words = [text.split() for text in texts]\n",
    "    words = [w for sub in words for w in sub]\n",
    "    \n",
    "    shingles = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        shingles.append(tuple(words[i:i+n]))\n",
    "        \n",
    "    return set(shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_duplicate(shingles1, shingles2, threshold=0.8):\n",
    "    union = len(set(list(shingles1) + list(shingles2)))\n",
    "    overlap = len(list(shingles1) + list(shingles2)) - union\n",
    "\n",
    "    return overlap/union > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "n_pages = 100\n",
    "\n",
    "q = Queue()\n",
    "seed_url = \"https://en.wikipedia.org/wiki/Alexandria_Ocasio-Cortez\"\n",
    "\n",
    "q.put(seed_url)\n",
    "\n",
    "# Save texts of processed pages for near-duplication detection\n",
    "processed = []\n",
    "\n",
    "while i <= n_pages:\n",
    "    # Get URL from queue\n",
    "    url = q.get()\n",
    "\n",
    "    # Make a request to URL\n",
    "    try:\n",
    "        req = requests.get(url, verify=False, timeout=5)\n",
    "    except Exception:\n",
    "        continue\n",
    "    \n",
    "    # Check if page exists and is accessible\n",
    "    if req.status_code != 200:\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(req.text)\n",
    "    \n",
    "    # Extract 4-shingles out of textual data\n",
    "    shingles = extract_shingles(soup, n=4)\n",
    "    \n",
    "    # Check for near duplicates among processed files\n",
    "    if any([near_duplicate(shingles, processed_file, threshold=0.8) for processed_file in processed]):\n",
    "        continue\n",
    "    \n",
    "    # Save shingles for near-duplicate detection\n",
    "    processed.append(shingles)\n",
    "    \n",
    "    # Insert the URL as a comment on the first line of the created file\n",
    "    soup.insert(0, '\\n')\n",
    "    soup.insert(0, Comment(url))\n",
    "    \n",
    "    # Save the HTML of retrieved file\n",
    "    with open('Pages/{}'.format('{}.html'.format(i)), 'wb+') as file:\n",
    "        file.write(soup.prettify('utf-8'))\n",
    "                \n",
    "    # Find all outlinks in the file\n",
    "    urls = find_outlinks(soup)\n",
    "    # Retrieve rules from robots.txt\n",
    "    rules = process_robots(url)\n",
    "    \n",
    "    # Use only outlinks that aren't contradictory to rules from robots.txt\n",
    "    urls_to_add = []\n",
    "    for u in urls:\n",
    "        for rule in rules:\n",
    "            if rule.startswith(u):\n",
    "                continue\n",
    "        urls_to_add.append(u)\n",
    "    \n",
    "    # Put filtered outlinks to queue\n",
    "    for u in urls_to_add:\n",
    "        q.put(u)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corners cut:\n",
    "- Some outlinks are discarded for easier processing\n",
    "- Crawler cares only about 'Disallow' rules in robots.txt for any user-agent\n",
    "- Near-duplicate detection is done using Jaccard similarity without using super-shingles\n",
    "- URLs are added into queue without reprioritizing, Mercator's scheme isn't implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for visible text only\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    elif element == '\\n' or element == ' ':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def tokenize_text(soup):\n",
    "    # Extract all textual data\n",
    "    data = soup.findAll(text=True)\n",
    "    \n",
    "    # Filter out comments, scripts, etc.\n",
    "    texts = filter(visible, data)\n",
    "    # Remove whitespaces\n",
    "    texts = [word_tokenize(str(e).strip()) for e in texts]\n",
    "    \n",
    "    # Convert to list of words\n",
    "    words = [w.lower() for sub in texts for w in sub]\n",
    "    \n",
    "    # Remove words of length one\n",
    "    words = [w for w in words if len(w) > 1]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files crawled in previous step\n",
    "files = glob('Pages/*')\n",
    "\n",
    "docterm = {}\n",
    "words = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        # Scan file content\n",
    "        soup = BeautifulSoup(f)\n",
    "\n",
    "        # Tokenize text\n",
    "        tokenized = tokenize_text(soup)\n",
    "\n",
    "        # Prepare english stoplist\n",
    "        stopwords_list = stopwords.words('english')\n",
    "\n",
    "        # Filter all words according to stoplist\n",
    "        filtered = [word for word in tokenized if word not in stopwords_list]\n",
    "\n",
    "        # Stem the rest of the words\n",
    "        stemmed = [ps.stem(word) for word in filtered]\n",
    "\n",
    "        # Save the list of stemmed words of the file\n",
    "        docterm[file.split('/')[-1].split('.')[0]] = stemmed\n",
    "        \n",
    "        # Keep all the seen words\n",
    "        words = words + stemmed\n",
    "\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final term-document matrix\n",
    "\n",
    "termdoc = {}\n",
    "\n",
    "for word in words:\n",
    "    termdoc[word] = []\n",
    "    \n",
    "    for id, dictionary in docterm.items():\n",
    "        if word in dictionary:\n",
    "            termdoc[word].append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the term-document matrix to a file\n",
    "\n",
    "output_file = 'BinaryInvertedIndex'\n",
    "\n",
    "with open(output_file, 'w+') as f:\n",
    "    for term, documents in termdoc.items():\n",
    "        f.write('{}: {}\\n'.format(term, documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corners cut:\n",
    "- Tokenizing doesn't check for language, encoding and assumes HTML format\n",
    "- No normalization is performed\n",
    "- Synonyms and homonyms aren't explicitly taken care of\n",
    "- Inverted index is implemented without positional indexes, i.e. doesn't explicitly handle bi-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ranking (content-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Creating multidimensional vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for visible text only\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    elif element == '\\n' or element == ' ':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def tokenize_text(soup):\n",
    "    # Extract all textual data\n",
    "    data = soup.findAll(text=True)\n",
    "    \n",
    "    # Filter out comments, scripts, etc.\n",
    "    texts = filter(visible, data)\n",
    "    # Remove whitespaces\n",
    "    texts = [word_tokenize(str(e).strip()) for e in texts]\n",
    "    \n",
    "    # Convert to list of words\n",
    "    words = [w.lower() for sub in texts for w in sub]\n",
    "    \n",
    "    # Remove words of length one\n",
    "    words = [w for w in words if len(w) > 1]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files crawled in previous step\n",
    "files = glob('Pages/*')\n",
    "\n",
    "docterm = {}\n",
    "words = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        # Scan file content\n",
    "        soup = BeautifulSoup(f)\n",
    "\n",
    "        # Tokenize text\n",
    "        tokenized = tokenize_text(soup)\n",
    "\n",
    "        # Prepare english stoplist\n",
    "        stopwords_list = stopwords.words('english')\n",
    "\n",
    "        # Filter all words according to stoplist\n",
    "        filtered = [word for word in tokenized if word not in stopwords_list]\n",
    "\n",
    "        # Stem the rest of the words\n",
    "        stemmed = [ps.stem(word) for word in filtered]\n",
    "\n",
    "        # Save the list of stemmed words of the file\n",
    "        docterm[file.split('/')[-1].split('.')[0]] = stemmed\n",
    "        \n",
    "        # Keep all the seen words\n",
    "        words = words + stemmed\n",
    "\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final term-document matrix\n",
    "\n",
    "termdoc = {}\n",
    "\n",
    "for word in words:\n",
    "    termdoc[word] = {}\n",
    "    \n",
    "    for id, dictionary in docterm.items():\n",
    "        if word in dictionary:\n",
    "            # Assign ID of the document with the count of the word\n",
    "            termdoc[word][id] = dictionary.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "N = len(files)\n",
    "# Document frequency of terms\n",
    "df_t = {k: len(v) for k, v in termdoc.items()}\n",
    "\n",
    "# Inverted document frequency of terms\n",
    "idf_t = {k: np.log10(N/v) for k, v in df_t.items()}\n",
    "\n",
    "# Recalculate term frequency\n",
    "tf_td = {}\n",
    "\n",
    "for term in termdoc.keys():\n",
    "    tf_td[term] = {k: (1+np.log10(v)) for k, v in termdoc[term].items()}\n",
    "\n",
    "# Calculate tf-idf\n",
    "tfidf = {}\n",
    "\n",
    "for term in tf_td.keys():\n",
    "    tfidf[term] = {k: (v*idf_t[term]) for k, v in tf_td[term].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Representing queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(q):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(q)\n",
    "    \n",
    "    # Apply stoplist\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    tokens_stopped = [t for t in tokens if t not in stopwords_list]\n",
    "    \n",
    "    # Stem\n",
    "    ps = PorterStemmer()\n",
    "    stems = [ps.stem(t) for t in tokens_stopped]\n",
    "    \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Alexandria Ocasio-Cortez'\n",
    "\n",
    "# Tokenize, stem and remove stopwords from query\n",
    "processed = process_query(query)\n",
    "\n",
    "# Term count\n",
    "term_count = {k: (1+np.log10(processed.count(k))) for k in processed}\n",
    "# Term weight basedd on idf_t\n",
    "term_weight = {k: ((v*idf_t[k]) if k in idf_t else 0) for k, v in term_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors for computing cosine similarity\n",
    "\n",
    "N = len(files)\n",
    "docterm_tfidf = {}\n",
    "\n",
    "for i in range(1, N+1):\n",
    "    docterm_tfidf[str(i)] = {k: (tfidf[k][str(i)] if str(i) in tfidf[k] else 0) for k in words}\n",
    "\n",
    "query_tfidf = {k: (term_weight[k] if k in term_weight else 0) for k in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Finding top-$k$ results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute cosine similarity between all documents and searched query\n",
    "\n",
    "similarities = cosine_similarity(\n",
    "    [list(v.values()) for v in docterm_tfidf.values()], \n",
    "    np.array(list(query_tfidf.values())).reshape(1, -1)\n",
    ")\n",
    "\n",
    "# Index similarities with documents IDs\n",
    "similarities_indexed = {str(i): similarities[i-1][0] for i in range(1, N+1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of results to return\n",
    "k = 10\n",
    "\n",
    "# All results sorted\n",
    "results = OrderedDict(sorted(similarities_indexed.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Top-k results sorted\n",
    "top_k_results = list(results.items())[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Best result:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages/11.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Other good results:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages/40.html\n",
      "Pages/73.html\n",
      "Pages/20.html\n",
      "Pages/16.html\n",
      "Pages/1.html\n",
      "Pages/86.html\n",
      "Pages/60.html\n",
      "Pages/9.html\n",
      "Pages/85.html\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "\n",
    "display(HTML('<b>Best result:</b>'))\n",
    "print('Pages/{}.html'.format(top_k_results[0][0]))\n",
    "display(HTML('<b>Other good results:</b>'))\n",
    "for page in top_k_results[1:]:\n",
    "    print('Pages/{}.html'.format(page[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(by considering only docs containing atleast one query term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(files)\n",
    "docterm_tfidf_p = {}\n",
    "\n",
    "# Skips documents that don't contain any query term\n",
    "for word in processed:\n",
    "    for i in range(1, N+1):\n",
    "        if str(i) in tfidf[word]:\n",
    "            docterm_tfidf_p[str(i)] = {k: (tfidf[k][str(i)] if str(i) in tfidf[k] else 0) for k in words}\n",
    "\n",
    "query_tfidf = {k: (term_weight[k] if k in term_weight else 0) for k in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_p = cosine_similarity(\n",
    "    [list(v.values()) for v in docterm_tfidf_p.values()], \n",
    "    np.array(list(query_tfidf.values())).reshape(1, -1)\n",
    ")\n",
    "\n",
    "# Index similarities with documents IDs\n",
    "similarities_indexed_p = {list(docterm_tfidf_p.keys())[i]: similarities_p[i][0] for i in range(0, len(similarities_p))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of results to return\n",
    "k = 10\n",
    "\n",
    "# All results sorted\n",
    "results = OrderedDict(sorted(similarities_indexed.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Top-k results sorted\n",
    "top_k_results = list(results.items())[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Best result:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages/11.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Other good results:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages/40.html\n",
      "Pages/73.html\n",
      "Pages/20.html\n",
      "Pages/16.html\n",
      "Pages/1.html\n",
      "Pages/86.html\n",
      "Pages/60.html\n",
      "Pages/9.html\n",
      "Pages/85.html\n"
     ]
    }
   ],
   "source": [
    "# Results are the same as without pruning\n",
    "\n",
    "display(HTML('<b>Best result:</b>'))\n",
    "print('Pages/{}.html'.format(top_k_results[0][0]))\n",
    "display(HTML('<b>Other good results:</b>'))\n",
    "for page in top_k_results[1:]:\n",
    "    print('Pages/{}.html'.format(page[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ranking (link-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Calculating PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('Pages/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_anchors_and_protocol(link):\n",
    "    if '#' in link:\n",
    "        link = link[:link.index('#')]\n",
    "    if '://' in link:\n",
    "        link = link[link.find('://') + len('://'):]\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save URLs of files to build a graph\n",
    "\n",
    "urls = {}\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        soup = BeautifulSoup(f)\n",
    "        urls[remove_anchors_and_protocol(soup.findAll(text = lambda text: isinstance(text, Comment))[0])] = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directed graph\n",
    "\n",
    "g = nx.DiGraph()\n",
    "g.add_nodes_from(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill graph with edges\n",
    "\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        soup = BeautifulSoup(f)\n",
    "        outlinks = find_outlinks(soup)\n",
    "        no_anchors = [remove_anchors_and_protocol(link) for link in outlinks]\n",
    "\n",
    "        for link in no_anchors:\n",
    "            if link in urls:\n",
    "                g.add_edge(file, urls[link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PageRank on the graph\n",
    "pr = nx.pagerank(g, alpha=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>PageRank:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages/2.html: 0.19729884849841883\n",
      "Pages/55.html: 0.043012226278560106\n",
      "Pages/10.html: 0.04177481739955622\n",
      "Pages/8.html: 0.025086382088624876\n",
      "Pages/67.html: 0.024866114825023117\n",
      "Pages/57.html: 0.023543162196014884\n",
      "Pages/53.html: 0.023171097779716536\n",
      "Pages/54.html: 0.022713330231858737\n",
      "Pages/56.html: 0.02215577088102767\n",
      "Pages/5.html: 0.020557085033451036\n"
     ]
    }
   ],
   "source": [
    "# Sort by PageRank and show top-k\n",
    "\n",
    "k = 10\n",
    "\n",
    "pr_sorted = OrderedDict(sorted(pr.items(), key=lambda x: x[1], reverse=True))\n",
    "top_k_pr = list(pr_sorted.items())[:k]\n",
    "\n",
    "display(HTML('<b>PageRank:</b>'))\n",
    "for page, pr_val in top_k_pr:\n",
    "    print('{}: {}'.format(page, pr_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Finding top-$k$ results considering PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Alexandria Ocasio-Cortez'\n",
    "\n",
    "# Tokenize, stem and remove stopwords from query\n",
    "processed = process_query(query)\n",
    "\n",
    "# Term count\n",
    "term_count = {k: (1+np.log10(processed.count(k))) for k in processed}\n",
    "# Term weight basedd on idf_t\n",
    "term_weight = {k: ((v*idf_t[k]) if k in idf_t else 0) for k, v in term_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(files)\n",
    "docterm_tfidf_p = {}\n",
    "\n",
    "# Skips documents that don't contain any query term\n",
    "for word in processed:\n",
    "    for i in range(1, N+1):\n",
    "        if str(i) in tfidf[word]:\n",
    "            docterm_tfidf_p[str(i)] = {k: (tfidf[k][str(i)] if str(i) in tfidf[k] else 0) for k in words}\n",
    "\n",
    "query_tfidf = {k: (term_weight[k] if k in term_weight else 0) for k in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_p = cosine_similarity(\n",
    "    [list(v.values()) for v in docterm_tfidf_p.values()], \n",
    "    np.array(list(query_tfidf.values())).reshape(1, -1)\n",
    ")\n",
    "\n",
    "# Index similarities with documents IDs\n",
    "similarities_indexed_p = {list(docterm_tfidf_p.keys())[i]: similarities_p[i][0] for i in range(0, len(similarities_p))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize similarities to have same weight as PageRank\n",
    "\n",
    "sim_max = max(list(similarities_indexed_p.values()))\n",
    "sim_min = min(list(similarities_indexed_p.values()))\n",
    "\n",
    "sim_norm = {'Pages/{}.html'.format(k): (v-sim_min)/(sim_max-sim_min) for k, v in similarities_indexed_p.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune and normalize PageRanks\n",
    "\n",
    "pr_pruned = {k: v for k, v in pr.items() if k in sim_norm.keys()}\n",
    "\n",
    "pr_max = max(list(pr_pruned.values()))\n",
    "pr_min = min(list(pr_pruned.values()))\n",
    "\n",
    "pr_norm = {k: (v-pr_min)/(pr_max-pr_min) for k, v in pr_pruned.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PageRank + Similarity\n",
    "sim_pr = {k: (v+pr_norm[k])/2 for k, v in sim_norm.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Best result:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages/40.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Other good results:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages/8.html\n",
      "Pages/11.html\n",
      "Pages/73.html\n",
      "Pages/33.html\n",
      "Pages/85.html\n",
      "Pages/16.html\n",
      "Pages/20.html\n",
      "Pages/13.html\n",
      "Pages/78.html\n"
     ]
    }
   ],
   "source": [
    "# Top-k results\n",
    "\n",
    "k = 10\n",
    "\n",
    "sim_pr_sorted = OrderedDict(sorted(sim_pr.items(), key=lambda x: x[1], reverse=True))\n",
    "top_k_sim_pr = list(sim_pr_sorted.items())[:k]\n",
    "\n",
    "display(HTML('<b>Best result:</b>'))\n",
    "print(top_k_sim_pr[0][0])\n",
    "display(HTML('<b>Other good results:</b>'))\n",
    "for page in top_k_sim_pr[1:]:\n",
    "    print(page[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Alexandria Ocasio-Cortez'\n",
    "\n",
    "# Tokenize, stem and remove stopwords from query\n",
    "processed = process_query(query)\n",
    "\n",
    "# Term count\n",
    "term_count = {k: (1+np.log10(processed.count(k))) for k in processed}\n",
    "# Term weight basedd on idf_t\n",
    "term_weight = {k: ((v*idf_t[k]) if k in idf_t else 0) for k, v in term_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(files)\n",
    "docterm_tfidf_p = {}\n",
    "\n",
    "# Skips documents that don't contain any query term\n",
    "for word in processed:\n",
    "    for i in range(1, N+1):\n",
    "        if str(i) in tfidf[word]:\n",
    "            docterm_tfidf_p[str(i)] = {k: (tfidf[k][str(i)] if str(i) in tfidf[k] else 0) for k in words}\n",
    "\n",
    "query_tfidf = {k: (term_weight[k] if k in term_weight else 0) for k in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_p = cosine_similarity(\n",
    "    [list(v.values()) for v in docterm_tfidf_p.values()], \n",
    "    np.array(list(query_tfidf.values())).reshape(1, -1)\n",
    ")\n",
    "\n",
    "# Index similarities with documents IDs\n",
    "similarities_indexed_p = {'Pages/{}.html'.format(list(docterm_tfidf_p.keys())[i]): similarities_p[i][0] for i in range(0, len(similarities_p))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of results to return\n",
    "t = 15\n",
    "\n",
    "# All results sorted\n",
    "results = OrderedDict(sorted(similarities_indexed_p.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Root sset\n",
    "top_t_results = list(results.items())[:t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only URLs that are in the root set\n",
    "urls_p = {k: v for k, v in urls.items() if v in [res[0] for res in top_t_results]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.DiGraph()\n",
    "g.add_nodes_from([res[0] for res in top_t_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add neighbours to create base set\n",
    "\n",
    "for file in [res[0] for res in top_t_results]:\n",
    "    with open(file, 'r') as f:\n",
    "        soup = BeautifulSoup(f)\n",
    "        outlinks = find_outlinks(soup)\n",
    "        no_anchors = [remove_anchors_and_protocol(link) for link in outlinks]\n",
    "\n",
    "        for link in no_anchors:\n",
    "            if link in urls:\n",
    "                urls_p[link] = urls[link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in urls.items():\n",
    "    with open(v, 'r') as f:\n",
    "        soup = BeautifulSoup(f)\n",
    "        outlinks = find_outlinks(soup)\n",
    "        no_anchors = [remove_anchors_and_protocol(link) for link in outlinks]\n",
    "\n",
    "        for link in no_anchors:\n",
    "            if link in urls_p:\n",
    "                g.add_edge(v, urls_p[link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HITS on the graph\n",
    "h, a = nx.hits(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Authorities:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages/55.html: 0.0477804784019622\n",
      "Pages/67.html: 0.035448733294485286\n",
      "Pages/56.html: 0.032744503507629176\n",
      "Pages/53.html: 0.03208615203694629\n",
      "Pages/2.html: 0.03167806686756724\n",
      "Pages/54.html: 0.031030548337235425\n",
      "Pages/5.html: 0.027779397744287778\n",
      "Pages/71.html: 0.02771408634762611\n",
      "Pages/8.html: 0.025749759710674114\n",
      "Pages/85.html: 0.021922514550108643\n"
     ]
    }
   ],
   "source": [
    "# Sort and print top-k authorities\n",
    "\n",
    "k = 10\n",
    "\n",
    "a_sorted = OrderedDict(sorted(a.items(), key=lambda x: x[1], reverse=True))\n",
    "top_k_a = list(a_sorted.items())[:k]\n",
    "    \n",
    "display(HTML('<b>Authorities:</b>'))\n",
    "for page, a_val in top_k_a:\n",
    "    print('{}: {}'.format(page, a_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Hubs:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages/1.html: 0.0687927726271976\n",
      "Pages/59.html: 0.03606287184992874\n",
      "Pages/67.html: 0.028116959793936167\n",
      "Pages/55.html: 0.026924234485432286\n",
      "Pages/48.html: 0.02515987441609795\n",
      "Pages/76.html: 0.024528906876039726\n",
      "Pages/61.html: 0.024236884618082682\n",
      "Pages/54.html: 0.024022396512391277\n",
      "Pages/71.html: 0.02377848620053737\n",
      "Pages/12.html: 0.022498951121742914\n"
     ]
    }
   ],
   "source": [
    "# Sort and print top-k hubs\n",
    "\n",
    "k = 10\n",
    "\n",
    "h_sorted = OrderedDict(sorted(h.items(), key=lambda x: x[1], reverse=True))\n",
    "top_k_h = list(h_sorted.items())[:k]\n",
    "\n",
    "display(HTML('<b>Hubs:</b>'))\n",
    "for page, h_val in top_k_h:\n",
    "    print('{}: {}'.format(page, h_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
